1. Build a simple GPT model implementation in PyTorch
2. Build a simple byte pair encoding tokenizer in go
3. Implement a cuda kernel for basic attention mechanism
4. Implement a cuda kernel for flash attention mechanism
5. Implement a cuda kernel for star attention mechanism
6. Implement distributed training for GPT model
7. Implement quantization for GPT model
8. Implement pruning for GPT model
