import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data.distributed import DistributedSampler
from torch.utils.data import Dataset, DataLoader
from gpt import GPTLanguageModel
import wandb
import os
import argparse
import logging
import time

def setup_logger(rank, batch_size, world_size, log_file=None):
    if log_file is None:
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        log_file = f'training_b{batch_size}_g{world_size}_{timestamp}.log'

    logger = logging.getLogger(f'rank_{rank}')
    logger.setLevel(logging.INFO)

    # File handler (shared between ranks)
    fh = logging.FileHandler(log_file)
    fh.setLevel(logging.INFO)

    # Format with rank information
    formatter = logging.Formatter('%(asctime)s - Rank %(rank)s - %(message)s')

    # Add rank to the log record
    class RankFilter(logging.Filter):
        def filter(self, record):
            record.rank = rank
            return True

    logger.addFilter(RankFilter())
    fh.setFormatter(formatter)
    logger.addHandler(fh)

    # Console handler (only for rank 0)
    if rank == 0:
        ch = logging.StreamHandler()
        ch.setLevel(logging.INFO)
        ch.setFormatter(formatter)
        logger.addHandler(ch)

    return logger

def parse_args():
    parser = argparse.ArgumentParser(description='Train GPT model with distributed training')
    parser.add_argument('--num_gpus', type=int, default=-1,
                       help='Number of GPUs to use. -1 means use all available GPUs')
    parser.add_argument('--batch_size', type=int, default=64,
                       help='Global batch size')
    parser.add_argument('--epochs', type=int, default=1,  # Changed from max_iters to epochs
                       help='Number of epochs to train')
    parser.add_argument('--learning_rate', type=float, default=3e-4,
                       help='Learning rate')
    parser.add_argument('--log_interval', type=int, default=500,
                       help='How often to log batch loss to wandb')
    return parser.parse_args()


def setup(rank, world_size):
    try:
        os.environ["MASTER_ADDR"] = "localhost"
        os.environ["MASTER_PORT"] = "12355"
        dist.init_process_group("nccl", rank=rank, world_size=world_size)
    except Exception as e:
        print(f"Failed to initialize process group: {e}")
        raise

def cleanup():
    try:
        dist.destroy_process_group()
    except Exception as e:
        print(f"Failed to cleanup process group: {e}")

class TextDataset(Dataset):
    def __init__(self, data, block_size):
        self.data = data
        self.block_size = block_size

    def __len__(self):
        return len(self.data) - self.block_size

    def __getitem__(self, idx):
        x = self.data[idx:idx+self.block_size]
        y = self.data[idx+1:idx+self.block_size+1]
        return x, y

def train(rank, world_size, args):
    # Setup logger
    logger = setup_logger(
            rank=rank,
            batch_size=args.batch_size,
            world_size=world_size
        )

    # Setup distributed training
    setup(rank, world_size)
    device = torch.device(f"cuda:{rank}")

    # Training Hyperparameters
    batch_size = args.batch_size // world_size  # Adjust batch size per GPU
    block_size = 256
    epochs = args.epochs
    learning_rate = args.learning_rate
    log_interval = args.log_interval
    eval_batches = 300
    n_embd = 384
    n_head = 6
    n_layer = 6
    dropout = 0.2

    # Initialize wandb only on rank 0
    if rank == 0:
        wandb.init(
            project="gpt-distributed",
            config={
                "batch_size": batch_size * world_size,
                "block_size": block_size,
                "epochs": epochs,
                "learning_rate": learning_rate,
                "eval_batches": eval_batches,
                "n_embd": n_embd,
                "n_head": n_head,
                "n_layer": n_layer,
                "dropout": dropout,
                "log_interval": log_interval,
            }
        )

    # Load and preprocess data
    with open("input.txt", "r", encoding="utf-8") as f:
        text = f.read()

    chars = sorted(list(set(text)))
    vocab_size = len(chars)
    stoi = {ch: i for i, ch in enumerate(chars)}
    itos = {i: ch for i, ch in enumerate(chars)}
    encode = lambda s: [stoi[c] for c in s]
    decode = lambda l: "".join([itos[i] for i in l])

    data = torch.tensor(encode(text), dtype=torch.long)
    n = int(0.9 * len(data))
    train_data = data[:n]
    val_data = data[n:]

    # Create datasets
    train_dataset = TextDataset(train_data, block_size)
    val_dataset = TextDataset(val_data, block_size)

    # Create samplers
    train_sampler = DistributedSampler(train_dataset, num_replicas=world_size, rank=rank)
    val_sampler = DistributedSampler(val_dataset, num_replicas=world_size, rank=rank)

    # Create Data Loaders
    train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=False,
            sampler=train_sampler,
        )
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        sampler=val_sampler,
    )

    # Create model and move to GPU
    model = GPTLanguageModel(vocab_size)
    model = model.to(device)
    model = DDP(model, device_ids=[rank])

    # Create optimizer
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

    @torch.no_grad()
    def estimate_loss():
        logger.info("Starting loss estimation on rank 0")
        out = {}
        model.eval()

        try:
            for split, loader in [('train', train_loader), ('val', val_loader)]:
                logger.info(f"Evaluating {split} split")
                losses = []
                for batch_idx, (x, y) in enumerate(loader):
                    if batch_idx >= eval_batches:  # Stop after eval_batches batches
                         logger.info(f"Reached {eval_batches} batches, stopping evaluation")
                         break
                    logger.info(f"Eval batch {batch_idx}")
                    x, y = x.to(device), y.to(device)
                    logits, loss = model(x, y)
                    losses.append(loss.item())
                    logger.info(f"Batch {batch_idx} loss: {loss.item():.4f}")

                avg_loss = sum(losses) / len(losses)
                logger.info(f"Average {split} loss: {avg_loss:.4f}")
                out[split] = avg_loss

        except Exception as e:
            logger.error(f"Error in loss estimation: {str(e)}")
            raise
        finally:
            model.train()

        return out

    def gather_metrics(rank, world_size, metrics_dict):
        try:
            dist.barrier()
            if world_size > 1:
                gathered_metrics = [None for _ in range(world_size)]
                for _ in range(3):  # Retry up to 3 times
                    try:
                        dist.all_gather_object(gathered_metrics, metrics_dict)
                        return gathered_metrics
                    except Exception as e:
                        logger.warning(f"Retry gathering metrics due to: {e}")
                        time.sleep(1)  # Wait before retry
                raise Exception("Failed to gather metrics after retries")
            else:
                return [metrics_dict]
        except Exception as e:
            logger.error(f"Error during gathering metrics: {e}")
            return None

    # Training loop
    logger.info("Starting training process")

    total_batches = len(train_loader)
    logger.info(f"Rank {rank}: Total batches per epoch: {total_batches}")

    # Global batch counter for continuous batch logging across epochs
    global_batch = 0

    for epoch in range(epochs):
        logger.info(f"Rank {rank}: Starting epoch {epoch}")
        train_sampler.set_epoch(epoch)  # Important for proper shuffling

        # Training phase
        model.train()
        total_loss = 0

        # Initialize timing counters
        data_loading_time = 0.0
        data_movement_time = 0.0
        training_compute_time = 0.0
        training_communication_time = 0.0

        # Start timing for data loading
        if device.type == "cuda":
            torch.cuda.synchronize()
        data_loading_start_time = time.perf_counter()

        for batch_idx, (x, y) in enumerate(train_loader):
            # End timing for data loading
            if device.type == "cuda":
                torch.cuda.synchronize()
            data_loading_end_time = time.perf_counter()
            data_loading_time += data_loading_end_time - data_loading_start_time

            # Start timing for data movement
            if device.type == "cuda":
                torch.cuda.synchronize()
            data_movement_start_time = time.perf_counter()

            x, y = x.to(device), y.to(device)

            if device.type == "cuda":
                torch.cuda.synchronize()
            data_movement_end_time = time.perf_counter()
            data_movement_time += data_movement_end_time - data_movement_start_time

            # Start timing for training compute
            if device.type == "cuda":
                torch.cuda.synchronize()
            training_compute_start = time.perf_counter()

            optimizer.zero_grad()
            logits, loss = model(x, y)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            if device.type == "cuda":
                torch.cuda.synchronize()
            training_compute_end = time.perf_counter()
            training_compute_time += training_compute_end - training_compute_start

            # Start timing for training communication/gradient sync
            if device.type == "cuda":
                torch.cuda.synchronize()
            comm_start = time.perf_counter()

            optimizer.step()

            if device.type == "cuda":
                torch.cuda.synchronize()
            comm_end = time.perf_counter()
            training_communication_time += comm_end - comm_start

            total_loss += loss.item()

            # Log batch loss from all ranks
            if batch_idx % log_interval == 0:
                logger.info(f'Rank {rank}: Epoch: {epoch}, Batch: {batch_idx}/{total_batches}, Loss: {loss.item():.4f}')
                # Log to wandb with rank information
                if rank == 0:
                    wandb.log({
                        "batch_loss": loss.item(),
                        "batch": global_batch,
                        "epoch": epoch,
                        "batch_in_epoch": batch_idx
                    })

            global_batch += 1
            # Reset data loading timer
            if device.type == "cuda":
                torch.cuda.synchronize()
            data_loading_start_time = time.perf_counter()

        # Calculate average loss for the epoch
        avg_loss = total_loss / total_batches

        # Total epoch time
        total_epoch_time = data_loading_time + data_movement_time + training_compute_time + training_communication_time

        # Log timing information
        logger.info(f"""
        Rank {rank} Epoch {epoch} timing breakdown:
            Total epoch time: {total_epoch_time:.2f} seconds
            Data loading time: {data_loading_time:.2f} seconds
            Data movement time: {data_movement_time:.2f} seconds
            Training compute time: {training_compute_time:.2f} seconds
            Training communication time: {training_communication_time:.2f} seconds
        """)

        # Gather losses and timings from all ranks
        # Ensure all processes are ready to gather
        dist.barrier()
        try:
            if world_size > 1:
                timing_info = {
                            'total_time': total_epoch_time,
                            'data_loading': data_loading_time,
                            'data_movement': data_movement_time,
                            'compute': training_compute_time,
                            'communication': training_communication_time
                        }

                all_timing_info = gather_metrics(rank, world_size, timing_info)
                avg_losses = gather_metrics(rank, world_size, avg_loss)

                if all_timing_info is None or avg_losses is None:
                    logger.error("Failed to gather metrics, skipping metrics logging for this epoch")
                    continue
            else:
                avg_losses = [avg_loss]
                all_timing_info = [{
                                'total_time': total_epoch_time,
                                'data_loading': data_loading_time,
                                'data_movement': data_movement_time,
                                'compute': training_compute_time,
                                'communication': training_communication_time
                            }]
        except Exception as e:
            logger.error(f"Error during gathering metrics: {e}")

        # Evaluation phase
        if rank == 0:
            losses = estimate_loss()
            logger.info(f"Epoch {epoch}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}")

            # Calculate and log average loss across all ranks
            epoch_avg_loss = sum(avg_losses) / len(avg_losses)

            # Calculate and log average times
            avg_timing = {
                            'total_time': sum(info['total_time'] for info in all_timing_info) / world_size,
                            'data_loading': sum(info['data_loading'] for info in all_timing_info) / world_size,
                            'data_movement': sum(info['data_movement'] for info in all_timing_info) / world_size,
                            'compute': sum(info['compute'] for info in all_timing_info) / world_size,
                            'communication': sum(info['communication'] for info in all_timing_info) / world_size
                        }
            wandb.log({
                "epoch": epoch,
                "train_loss": losses['train'],
                "val_loss": losses['val'],
                "avg_training_loss": epoch_avg_loss,
                "epoch_completed": epoch + 1,
                'avg_epoch_total_time': avg_timing['total_time'],
                'avg_epoch_data_loading_time': avg_timing['data_loading'],
                'avg_epoch_data_movement_time': avg_timing['data_movement'],
                'avg_epoch_compute_time': avg_timing['compute'],
                'avg_epoch_communication_time': avg_timing['communication'],
            })

        # Instead, gather metrics to rank 0 and log them there
        if rank == 0:
            for r in range(world_size):
                wandb.log({
                    f"epoch_avg_loss_rank_{r}": avg_losses[r],
                    "epoch": epoch,
                    f"epoch_total_time_rank_{r}": all_timing_info[r]['total_time'],
                    f"epoch_data_loading_time_rank_{r}": all_timing_info[r]['data_loading'],
                    f"epoch_data_movement_time_rank_{r}": all_timing_info[r]['data_movement'],
                    f"epoch_compute_time_rank_{r}": all_timing_info[r]['compute'],
                    f"epoch_communication_time_rank_{r}": all_timing_info[r]['communication'],
                })

        # Synchronize before starting next epoch
        dist.barrier()

    # Save model and generate text only on rank 0
    if rank == 0:
        os.makedirs('checkpoints', exist_ok=True)
        checkpoint_path = os.path.join('checkpoints', 'final_model.pth')
        torch.save(model.module.state_dict(), checkpoint_path)
        wandb.save(checkpoint_path, base_path=os.getcwd())

        vocab_path = os.path.join('checkpoints', 'vocab_info.pth')
        torch.save({'stoi': stoi, 'itos': itos}, vocab_path)
        wandb.save(vocab_path, base_path=os.getcwd())

        # Generate sample text
        context = torch.zeros((1, 1), dtype=torch.long, device=rank)
        generated_text = decode(model.module.generate(context, max_new_tokens=500)[0].tolist())
        wandb.log({"generated_text": generated_text})

        wandb.finish()

    # Clean up
    cleanup()


if __name__ == "__main__":
    args = parse_args()

    # Determine number of GPUs to use
    available_gpus = torch.cuda.device_count()
    if args.num_gpus == -1:
        world_size = available_gpus
    else:
        world_size = min(args.num_gpus, available_gpus)
        if world_size < args.num_gpus:
            print(f"Warning: Requested {args.num_gpus} GPUs but only {available_gpus} are available.")

    if world_size == 0:
        raise ValueError("No GPUs available for training!")

    print(f"Training with {world_size} GPUs")

    try:
        torch.multiprocessing.spawn(
            train,
            args=(world_size, args),
            nprocs=world_size,
            join=True
        )
    except Exception as e:
        print(f"Error during training: {e}")
        # Clean up in case of error
        if dist.is_initialized():
            cleanup()
